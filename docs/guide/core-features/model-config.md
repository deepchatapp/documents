# 模型配置

模型配置是提升 DeepChat 对话质量和效率的关键。本页面将详细介绍如何配置和优化不同的语言模型参数，以获得最佳的 AI 回复。

## 模型控制面板 (MCP)

Model Control Panel (MCP) 是 DeepChat 的核心功能，它提供了对模型行为的精细控制。

### 访问 MCP

打开模型控制面板的方法：

1. 在对话界面中，点击输入框右侧的"设置"图标
2. 使用快捷键 `Ctrl+Shift+M`（Windows/Linux）或 `Cmd+Shift+M`（macOS）
3. 在新建对话时选择"高级设置"

![模型控制面板](https://deepchat.thinkinai.xyz/chat-screenshot.png)

*这里应放置一张模型控制面板的截图，显示完整的参数设置界面，包括系统提示词、温度设置等关键参数。*

## 通用参数设置

以下是适用于大多数语言模型的通用参数：

### 温度 (Temperature)

控制回复的随机性和创造性：

- **值范围**：0.0 - 2.0
- **低温度** (0.0 - 0.3)：生成更确定、一致且可预测的回复
- **中温度** (0.4 - 0.7)：平衡创造性和确定性
- **高温度** (0.8 - 2.0)：生成更多样化、创新但可能不太准确的回复

::: tip 应用场景
- 代码生成、事实查询：使用低温度
- 一般对话、解释概念：使用中温度
- 创意写作、头脑风暴：使用高温度
:::

### 最大长度 (Max Length)

控制回复的最大长度：

- **值范围**：通常从 100 到 4000 tokens
- **短回复** (100-500)：简短、直接的答案
- **中等回复** (500-2000)：详细解释和分析
- **长回复** (2000+)：深入探讨或复杂内容创作

::: warning 注意
过长的最大长度会延长生成时间，但设置过短可能导致回答不完整。
:::

### 上下文窗口 (Context Window)

控制模型能"记住"的对话历史长度：

- **值范围**：取决于模型，通常为 2K - 128K tokens
- **小窗口** (2K-8K)：减少延迟和成本
- **中等窗口** (8K-32K)：平衡性能和上下文理解
- **大窗口** (32K+)：深度记忆长对话和复杂文档

### 系统提示词 (System Prompt)

为模型设置行为指南和角色定义：

```
你是一位专业的助手，名为DeepChat。
你的回答应该简洁、准确、有帮助。
当被要求编写代码时，请使用适当的语法高亮和注释。
如果不确定，请坦诚表明，而不是提供可能不准确的信息。
```

#### 系统提示词最佳实践

- **明确角色**：清晰定义AI的身份和专业领域
- **设置界限**：指明AI能做和不能做的事情
- **定义输出格式**：指定回答的结构和风格
- **包含关键指令**：添加重要的行为规则和限制
- **保持精简**：避免冗长的提示词，专注于关键指示

## 特定模型参数

不同模型可能有特定的参数设置：

### OpenAI 模型参数

- **Top P (核心采样)**：控制输出的多样性，值范围 0.0 - 1.0
- **Frequency Penalty (频率惩罚)**：减少重复使用相同词语的倾向，值范围 0.0 - 2.0
- **Presence Penalty (存在惩罚)**：增加引入新主题的可能性，值范围 0.0 - 2.0
- **Stop Sequences (停止序列)**：指定模型停止生成的特定文本标记

### Claude 模型参数

- **Top K**：限制每一步考虑的最可能tokens数量
- **Claude Messages**：配置Claude特有的消息格式

### 本地模型参数

- **Repeat Penalty (重复惩罚)**：控制模型避免重复内容的程度
- **Attention Layers (注意力层)**：配置使用的注意力层数
- **Batch Size (批处理大小)**：影响生成速度和内存使用

## 参数预设

### 使用预设

DeepChat 提供了常用场景的参数预设：

1. 点击 MCP 面板中的"预设"按钮
2. 从列表中选择预设，如：
   - **精确回答**：低温度，高精度设置
   - **创意写作**：高温度，低约束设置
   - **代码生成**：低温度，优化代码输出
   - **长文档分析**：大上下文窗口，优化检索设置

### 创建自定义预设

保存您自己的参数组合：

1. 调整所有参数到理想状态
2. 点击"保存预设"按钮
3. 命名预设并添加描述
4. 可选：设为默认预设

## 多模型配置

### 模型切换

在同一对话中切换不同模型：

1. 点击对话窗口顶部的模型名称
2. 从下拉菜单中选择新模型
3. 确认是否应用默认参数或保留当前参数

::: tip 技巧
不同模型擅长不同任务，可根据需要切换：
- 代码生成：选择 GPT-4 或 Claude 3 Opus
- 创意内容：选择 Claude 3 Sonnet 或 GPT-4
- 快速回复：选择 GPT-3.5-Turbo 或本地轻量模型
:::

### 模型组合

配置多个模型协同工作：

1. 在设置中启用"链式模型"功能
2. 定义模型链，如让一个模型生成内容，另一个审核或优化
3. 设置触发条件和参数传递规则

![模型链配置](https://deepchat.thinkinai.xyz/chat-screenshot.png)

*这里应放置一张展示模型链设置界面的截图，显示两个或多个模型如何串联配置，以及参数传递和触发规则设置。*

## 高级配置技巧

### 微调模型行为

1. **混合参数调整**：同时调整多个相关参数获得最佳效果
   - 提高温度同时增加频率惩罚可获得创新但不重复的回复
   - 降低温度同时扩大上下文窗口可获得一致且有上下文理解的回答

2. **自适应系统提示词**：根据对话发展动态调整系统提示词
   - 点击对话设置中的"动态提示词"选项
   - 设置触发条件和新的系统提示词

### 配置文件管理

保存和管理完整的配置文件：

1. 设置好完整的模型配置
2. 在设置菜单中选择"导出配置"
3. 选择保存位置和文件名
4. 通过"导入配置"选项在其他设备上应用相同设置

## 性能优化

### 减少延迟和成本

- **选择适当的模型**：简单任务使用更小的模型，如GPT-3.5而非GPT-4
- **优化上下文窗口**：只保留必要的上下文历史
- **使用本地模型**：对于不需要最高性能的任务，考虑使用本地部署的开源模型

### 批量处理

处理大量类似任务时的优化：

1. 启用"批量处理"功能
2. 设置任务列表和处理参数
3. 配置错误处理和重试策略

## 故障排除

| 问题 | 解决方案 |
|------|---------|
| 模型响应过慢 | 减小上下文窗口；降低最大长度；切换到轻量级模型 |
| 回复质量不佳 | 优化系统提示词；调整温度参数；尝试不同模型 |
| 参数设置无效 | 检查模型是否支持该参数；查看API限制；重新登录账户 |
| 预设无法保存 | 检查存储权限；清理缓存；使用备份功能 |
| API额度超限 | 查看使用记录；设置使用限制；优化参数减少消耗 |

## 参数参考表

| 参数名称 | 值范围 | 推荐值 | 适用模型 | 影响 |
|---------|------|-------|-------|------|
| 温度 | 0.0 - 2.0 | 0.7 | 全部 | 控制多样性和创造性 |
| Top P | 0.0 - 1.0 | 0.9 | GPT系列，Claude | 控制随机性的另一种方式 |
| 最大长度 | 100 - 4000+ | 2000 | 全部 | 控制回复长度上限 |
| 频率惩罚 | 0.0 - 2.0 | 0.5 | GPT系列 | 减少重复内容 |
| 存在惩罚 | 0.0 - 2.0 | 0.5 | GPT系列 | 鼓励提及新概念 |
| 上下文窗口 | 2K - 128K | 8K | 全部 | 控制考虑的历史长度 |

下一步，您可以了解[提示词工程](./prompt-engineering.md)技巧，学习如何编写更有效的提示词来引导AI。 